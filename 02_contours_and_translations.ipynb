{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Contours and Translations for DAVIS 2016 Dataset\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import src.config as cfg\n",
    "from src.vis_utils import extract_longest_contour, load_gray_img\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_translations(contour_0, contour_1):\n",
    "    '''Returns the translations for each point in contour_0 to contour_1.'''\n",
    "    \n",
    "    contour_0 = np.squeeze(contour_0)\n",
    "    contour_1 = np.squeeze(contour_1)\n",
    "    \n",
    "    translations = contour_1 - contour_0\n",
    "    \n",
    "    translations_normalized = normalize(translations)\n",
    "    \n",
    "    return translations_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_annotation_with_contour_and_translation(annotation, contour, translation, path):\n",
    "    '''Saves annotation plotted with contour and translation to a given path.'''\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    \n",
    "    # Plot contour\n",
    "    contour = np.squeeze(contour)\n",
    "    plt.scatter(contour[:, 0], contour[:, 1])\n",
    "    \n",
    "    # Plot annotation\n",
    "    plt.imshow(annotation)\n",
    "    \n",
    "    # Plot translation\n",
    "    for c, t in zip(contour, translation):\n",
    "        plt.arrow(c[0], c[1],\n",
    "                  t[0], t[1],\n",
    "                  width=1, color='r')  \n",
    "    \n",
    "    # Save image\n",
    "    plt.savefig(path, bbox_inches=0)    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(contour_point_1, contour_1_real):\n",
    "    contour_1_real = np.squeeze(contour_1_real, axis=1)\n",
    "    dist = cdist(contour_point_1, contour_1_real, metric='euclidean')\n",
    "    arg_contour_point_1_final = np.argmin(dist)\n",
    "    contour_point_1_final = contour_1_real[arg_contour_point_1_final]\n",
    "    #print('contour_point_1:', contour_point_1, '--> contour_point_1_final:', contour_point_1_final)\n",
    "    return contour_point_1_final\n",
    "    \n",
    "def match_points(contour_1, contour_1_real):\n",
    "    '''Match points based on minimal distance. 2 points can be matched to the same point if it is the closest'''\n",
    "    rows_1, _, _ = contour_1.shape\n",
    "    contour_1_final = np.zeros((rows_1, 2))\n",
    "    for x in range(rows_1):\n",
    "        contour_1_final[x] = find_closest(contour_1[x], contour_1_real)\n",
    "    return contour_1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contours_and_translations(annotations_folders_path,\n",
    "                                     contours_folders_path,\n",
    "                                     translations_folders_path,\n",
    "                                     closing_kernel_size):\n",
    "\n",
    "    # Get list of sequences\n",
    "    sequences = os.listdir(annotations_folders_path)\n",
    "    sequences.sort()\n",
    "    \n",
    "    # Iterate through sequences\n",
    "    for i, sequence in enumerate(sequences):\n",
    "\n",
    "        print('#{}: {}'.format(i, sequence))\n",
    "        \n",
    "        if sequence not in cfg.TRAIN_SEQUENCES:\n",
    "            augmentation_count = 1\n",
    "        else:\n",
    "            augmentation_count = cfg.AUGMENTATION_COUNT + 1\n",
    "                                             \n",
    "        # Iterate through augmentations:\n",
    "        for j in range(augmentation_count):\n",
    "            \n",
    "            j = str(j)\n",
    "            \n",
    "            print('\\t{} #{}'.format('Augmentation', j))\n",
    "        \n",
    "            # Create folder to save Contours\n",
    "            contours_folder_path = os.path.join(contours_folders_path, sequence, j)\n",
    "            if not os.path.exists(contours_folder_path):\n",
    "                os.makedirs(contours_folder_path)\n",
    "                \n",
    "            # Create folder to save Translations\n",
    "            translations_folder_path = os.path.join(translations_folders_path, sequence, j)\n",
    "            if not os.path.exists(translations_folder_path):\n",
    "                os.makedirs(translations_folder_path)\n",
    "\n",
    "            # Get list of frames\n",
    "            if os.path.exists(os.path.join(annotations_folders_path, sequence, j)):\n",
    "                frames = os.listdir(os.path.join(annotations_folders_path, sequence, j))\n",
    "                if '.ipynb_checkpoints' in frames:\n",
    "                    frames.remove('.ipynb_checkpoints')\n",
    "                frames.sort()\n",
    "\n",
    "                # Iterate through frames\n",
    "                for k, frame in enumerate(frames):\n",
    "\n",
    "                    #print('\\t\\t#{}: {}'.format(k, frame))\n",
    "\n",
    "                    if (sequence == 'bmx-bumps' and frame == '00059.png'): break\n",
    "                    if (sequence == 'surf' and frame == '00053.png'): break\n",
    "\n",
    "                    # Get path to frames\n",
    "                    annotation_0_path = os.path.join(annotations_folders_path, sequence, j, frame)\n",
    "                    try:\n",
    "                        annotation_1_path = os.path.join(annotations_folders_path, sequence, j, frames[k+1])\n",
    "                    # Break if frame_0 is last frame\n",
    "                    except IndexError as e:\n",
    "                        break\n",
    "\n",
    "                    # Load frames as gray img\n",
    "                    annotation_0_gray = load_gray_img(annotation_0_path)\n",
    "                    annotation_1_gray = load_gray_img(annotation_1_path)\n",
    "\n",
    "                    # Extract longest contour and save it\n",
    "                    contour_0 = extract_longest_contour(annotation_0_gray, closing_kernel_size, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "                    np.save(os.path.join(contours_folder_path, frame[:5]), contour_0)\n",
    "\n",
    "                    # Calculate optical flow to get contour_1\n",
    "                    contour_1, st, err = cv2.calcOpticalFlowPyrLK(annotation_0_gray, annotation_1_gray, \n",
    "                                                                  contour_0, None, **lk_params)\n",
    "\n",
    "                    translation_0_1_normalized = get_normalized_translations(contour_0, contour_1)\n",
    "\n",
    "                    # Update contour_1 with normalized translations\n",
    "                    contour_1 = np.add(np.squeeze(contour_0), translation_0_1_normalized)\n",
    "                    contour_1 = np.expand_dims(contour_1, axis=1)\n",
    "\n",
    "                    contour_1_real = extract_longest_contour(annotation_1_gray, closing_kernel_size, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "                    # Match contour 1 on contour 1 real\n",
    "                    contour_1_final = match_points(contour_1, contour_1_real)\n",
    "\n",
    "                    # Get translation and save it\n",
    "                    translation_0_1_final = contour_1_final - np.squeeze(contour_0)\n",
    "\n",
    "                    correction = np.mean(np.linalg.norm(translation_0_1_normalized-translation_0_1_final, axis=1))\n",
    "\n",
    "                    np.save(os.path.join(translations_folder_path, frame[:5]), translation_0_1_final)\n",
    "\n",
    "                    # Save annotation with contour and translation\n",
    "                    #annotation = cv2.imread(os.path.join(annotations_folders_path, sequence, j, frame))\n",
    "                    #save_annotation_with_contour_and_translation(annotation, contour_0, translation_0_1_final, \n",
    "                    #    os.path.join(translations_folder_path, frame[:5] + '.png'))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Contours and Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: bear\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#1: blackswan\n",
      "\tAugmentation #0\n",
      "#2: bmx-bumps\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#3: bmx-trees\n",
      "\tAugmentation #0\n",
      "#4: boat\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#5: breakdance\n",
      "\tAugmentation #0\n",
      "#6: breakdance-flare\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#7: bus\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#8: camel\n",
      "\tAugmentation #0\n",
      "#9: car-roundabout\n",
      "\tAugmentation #0\n",
      "#10: car-shadow\n",
      "\tAugmentation #0\n",
      "#11: car-turn\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#12: cows\n",
      "\tAugmentation #0\n",
      "#13: dance-jump\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#14: dance-twirl\n",
      "\tAugmentation #0\n",
      "#15: dog\n",
      "\tAugmentation #0\n",
      "#16: dog-agility\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#17: drift-chicane\n",
      "\tAugmentation #0\n",
      "#18: drift-straight\n",
      "\tAugmentation #0\n",
      "#19: drift-turn\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#20: elephant\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#21: flamingo\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#22: goat\n",
      "\tAugmentation #0\n",
      "#23: hike\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#24: hockey\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#25: horsejump-high\n",
      "\tAugmentation #0\n",
      "#26: horsejump-low\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#27: kite-surf\n",
      "\tAugmentation #0\n",
      "#28: kite-walk\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#29: libby\n",
      "\tAugmentation #0\n",
      "#30: lucia\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#31: mallard-fly\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#32: mallard-water\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#33: motocross-bumps\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#34: motocross-jump\n",
      "\tAugmentation #0\n",
      "#35: motorbike\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#36: paragliding\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#37: paragliding-launch\n",
      "\tAugmentation #0\n",
      "#38: parkour\n",
      "\tAugmentation #0\n",
      "#39: rhino\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#40: rollerblade\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#41: scooter-black\n",
      "\tAugmentation #0\n",
      "#42: scooter-gray\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#43: soapbox\n",
      "\tAugmentation #0\n",
      "#44: soccerball\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#45: stroller\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#46: surf\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#47: swing\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#48: tennis\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n",
      "#49: train\n",
      "\tAugmentation #0\n",
      "\tAugmentation #1\n",
      "\tAugmentation #2\n",
      "\tAugmentation #3\n",
      "\tAugmentation #4\n",
      "\tAugmentation #5\n"
     ]
    }
   ],
   "source": [
    "create_contours_and_translations(cfg.ANNOTATIONS_AUGMENTED_FOLDERS_PATH,\n",
    "                                 cfg.CONTOURS_FOLDERS_PATH,\n",
    "                                 cfg.TRANSLATIONS_FOLDERS_PATH,\n",
    "                                 cfg.CLOSING_KERNEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
