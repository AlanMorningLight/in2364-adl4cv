{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN for DAVIS 2016\n",
    "\n",
    "In this notebook, a custom [PyTorch Geometric](https://rusty1s.github.io/pytorch_geometric/build/html/index.html) [InMemoryDataset](https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric/data/in_memory_dataset.html#InMemoryDataset) for the DAVIS 2016 dataset is created. The implementation is based on this [tutorial](https://rusty1s.github.io/pytorch_geometric/build/html/notes/create_dataset.html). The dataset is then used to train a simple GCN network as a first evaluation based on this [tutorial](https://rusty1s.github.io/pytorch_geometric/build/html/notes/introduction.html#learning-methods-on-graphs).\n",
    "\n",
    "The dataset consists of single PyTorch Geometric [Data](https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric/data/data.html#Data) objects which model a single graph with various attributes. For this dataset, a graph for each contour is created. Hereby, each node of the graph represents one contour point. The feature of each node is the OSVOS feature vector from the next frame at this point. Each node is connected to its K nearest neighbours. The feature of each edge is the distance between the nodes it connects. The targets of each node is the translation it undergoes from the current to the next frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from pg_datasets.davis_2016 import DAVIS2016\n",
    "from pg_networks.gcn import GCN\n",
    "from pg_solvers.solver import Solver\n",
    "from pg_utils.vis_utils import show_img_with_contour, plot_translation, show_loss\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_GEOMETRIC_DAVIS_2016_DATASET_PATH = 'pg_datasets/DAVIS_2016'#'pg_datasets/DAVIS_2016'\n",
    "CONTOURS_FOLDERS_PATH = 'DAVIS_2016/DAVIS/Contours/480p'\n",
    "IMAGES_FOLDERS_PATH = 'DAVIS_2016/DAVIS/JPEGImages/480p'\n",
    "TRANSLATIONS_FOLDERS_PATH = 'DAVIS_2016/DAVIS/Translations/480p'\n",
    "\n",
    "SKIP_SEQUENCES = []\n",
    "\n",
    "SKIP_SEQUENCES = ['bmx-trees', 'bus', 'cows', 'dog-agility', 'horsejump-high', \n",
    "                  'horsejump-low', 'kite-walk', 'lucia', 'libby', 'motorbike',\n",
    "                  'paragliding', 'rhino', 'scooter-gray', 'swing']\n",
    "\n",
    "TRAIN_SEQUENCES = ['bmx-bumps', 'boat', 'breakdance-flare', 'bus', \n",
    "                   'car-turn', 'dance-jump', 'dog-agility', 'drift-turn', \n",
    "                   'elephant', 'flamingo', 'hike', 'hockey', 'horsejump-low', \n",
    "                   'kite-walk', 'lucia', 'mallard-fly', 'mallard-water', \n",
    "                   'motocross-bumps', 'motorbike', 'paragliding', 'rhino', \n",
    "                   'rollerblade', 'scooter-gray', 'soccerball', 'stroller',\n",
    "                   'surf', 'swing', 'tennis', 'train']\n",
    "\n",
    "VAL_SEQUENCES = ['blackswan', 'bmx-trees', 'breakdance', 'camel', 'car-roundabout',\n",
    "                 'car-shadow', 'cows', 'dance-twirl', 'dog', 'drift-chicane', \n",
    "                 'drift-straight', 'goat', 'horsejump-high', 'kite-surf', 'libby', \n",
    "                 'motocross-jump', 'paragliding-launch', 'parkour', 'scooter-black', \n",
    "                 'soapbox']\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LAYER = 9\n",
    "K = 32\n",
    "EPOCHS_WO_AVEGRAD = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "#1: bmx-bumps\n",
      "['/home/christoph/in2364-adl4cv', '/opt/anaconda3/lib/python37.zip', '/opt/anaconda3/lib/python3.7', '/opt/anaconda3/lib/python3.7/lib-dynload', '', '/home/christoph/.local/lib/python3.7/site-packages', '/opt/anaconda3/lib/python3.7/site-packages', '/opt/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/home/christoph/.ipython', '/home/christoph/in2364-adl4cv/OSVOS_PyTorch']\n",
      "Start online training...\n",
      "Constructing OSVOS architecture..\n",
      "Initializing weights..\n",
      "Done initializing train_seqs Dataset\n",
      "Done initializing val_seqs Dataset\n",
      "Start of Online Training, sequence: bmx-bumps\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n",
      "torch.Size([1, 3, 480, 854])\n",
      "torch.Size([1, 1, 480, 854])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b08cf0612a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mLAYER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_WO_AVEGRAD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mSKIP_SEQUENCES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_SEQUENCES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_SEQUENCES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                   train=True)\n\u001b[0m",
      "\u001b[0;32m~/in2364-adl4cv/pg_datasets/davis_2016.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, contours_folders_path, images_folders_path, translations_folders_path, layer, k, epochs_wo_avegrad, skip_sequences, train_sequences, val_sequences, train, transform, pre_transform)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDAVIS2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/in2364-adl4cv/pg_datasets/davis_2016.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start online training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SEQ_NAME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_wo_avegrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished online training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/in2364-adl4cv/OSVOS_PyTorch/train_online.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_wo_avegrad)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Compute the fuse loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_balanced_cross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mrunning_loss_tr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# PyTorch 0.4.0 style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;31m# Print stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnEpochs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = DAVIS2016(PYTORCH_GEOMETRIC_DAVIS_2016_DATASET_PATH, \n",
    "                  CONTOURS_FOLDERS_PATH, IMAGES_FOLDERS_PATH, TRANSLATIONS_FOLDERS_PATH, \n",
    "                  LAYER, K, EPOCHS_WO_AVEGRAD,\n",
    "                  SKIP_SEQUENCES, TRAIN_SEQUENCES, VAL_SEQUENCES,\n",
    "                  train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = DAVIS2016(PYTORCH_GEOMETRIC_DAVIS_2016_DATASET_PATH, \n",
    "                CONTOURS_FOLDERS_PATH, IMAGES_FOLDERS_PATH, TRANSLATIONS_FOLDERS_PATH, \n",
    "                LAYER, K, EPOCHS_WO_AVEGRAD,\n",
    "                SKIP_SEQUENCES, TRAIN_SEQUENCES, VAL_SEQUENCES,\n",
    "                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train))\n",
    "print(\"Val size: %i\" % len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_display = 2\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    \n",
    "    # randomly select a sample\n",
    "    rand_i = np.random.randint(0, len(train))\n",
    "    sample = train[rand_i]\n",
    "    show_img_with_contour(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(in_channels=train[0].num_features, \n",
    "            out_channels=train[0].y.shape[1])\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 80\n",
    "num_val = 20\n",
    "\n",
    "overfit_train_loader = DataLoader(train, batch_size=16, \n",
    "                                  shuffle=False, sampler=SequentialSampler(range(num_train)))\n",
    "overfit_val_loader = DataLoader(train, batch_size=1, \n",
    "                                shuffle=False, sampler=SequentialSampler(range(num_val)))\n",
    "\n",
    "# Load model and run the solver\n",
    "overfit_model = GCN(in_channels=train[0].num_features, \n",
    "                    out_channels=train[0].y.shape[1])\n",
    "\n",
    "overfit_solver = Solver(optim_args={\"lr\": 1e-4})\n",
    "overfit_solver.train(overfit_model, overfit_train_loader, overfit_val_loader,\n",
    "                     num_epochs=20, log_nth=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(overfit_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.double()\n",
    "model.eval()\n",
    "num_to_display = 2\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    \n",
    "    rand_i = np.random.randint(0, num_val)\n",
    "    print(rand_i)\n",
    "    sample = train[rand_i]\n",
    "    \n",
    "    img = sample.img.detach().numpy().astype(np.int64)\n",
    "    img = np.squeeze(img)\n",
    "    img = np.moveaxis(img, 0, 2)\n",
    "    \n",
    "    translation = sample.y.detach().numpy()\n",
    "    \n",
    "    contour = sample.contour.detach().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_translation = model(sample).detach().numpy()\n",
    "\n",
    "    translation_ground_truth = contour + translation\n",
    "    translation_pred = contour + pred_translation\n",
    "    \n",
    "    plot_translation(img, translation_ground_truth, translation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load model and run the solver\n",
    "model = GCN(in_channels=train[0].num_features, \n",
    "            out_channels=train[0].y.shape[1])\n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 1e-4})\n",
    "\n",
    "solver.train(model, train_loader, val_loader,\n",
    "             num_epochs=20, log_nth=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display trained outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "num_to_display = 2\n",
    "for i in range(num_to_display):\n",
    "    \n",
    "    rand_i = np.random.randint(0, len(val))\n",
    "    sample = val[rand_i]\n",
    "    \n",
    "    img = sample.img.detach().numpy().astype(np.int64)\n",
    "    img = np.squeeze(img)\n",
    "    img = np.moveaxis(img, 0, 2)\n",
    "    \n",
    "    translation = sample.y.detach().numpy()\n",
    "    \n",
    "    contour = sample.contour.detach().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_translation = model(sample).detach().numpy()\n",
    "\n",
    "    translation_ground_truth = contour + translation\n",
    "    translation_pred = contour + pred_translation\n",
    "    \n",
    "    plot_translation(img, translation_ground_truth, translation_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
