{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN for DAVIS 2016\n",
    "\n",
    "In this notebook, a custom [PyTorch Geometric](https://rusty1s.github.io/pytorch_geometric/build/html/index.html) [InMemoryDataset](https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric/data/in_memory_dataset.html#InMemoryDataset) for the DAVIS 2016 dataset is created. The implementation is based on this [tutorial](https://rusty1s.github.io/pytorch_geometric/build/html/notes/create_dataset.html). The dataset is then used to train a simple GCN network as a first evaluation based on this [tutorial](https://rusty1s.github.io/pytorch_geometric/build/html/notes/introduction.html#learning-methods-on-graphs).\n",
    "\n",
    "The dataset consists of single PyTorch Geometric [Data](https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric/data/data.html#Data) objects which model a single graph with various attributes. For this dataset, a graph for each contour is created. Hereby, each node of the graph represents one contour point. The feature of each node is the OSVOS feature vector from the next frame at this point. Each node is connected to its K nearest neighbours. The feature of each edge is the distance between the nodes it connects. The targets of each node is the translation it undergoes from the current to the next frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from pg_datasets.davis_2016 import DAVIS2016\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_GEOMETRIC_DAVIS_2016_DATASET_PATH = 'pg_datasets/DAVIS_2016'\n",
    "CONTOURS_FOLDERS_PATH = 'DAVIS_2016/DAVIS/Contours/480p'\n",
    "IMAGES_FOLDERS_PATH = 'DAVIS_2016/DAVIS/JPEGImages/480p'\n",
    "TRANSLATIONS_FOLDERS_PATH = 'DAVIS_2016/DAVIS/Translations/480p'\n",
    "\n",
    "LAYER = 9\n",
    "K = 32\n",
    "\n",
    "SKIP_SEQUENCES = ['bmx-trees', 'bus', 'cows', 'dog-agility', 'horsejump-high', \n",
    "                  'horsejump-low', 'kite-walk', 'lucia', 'libby', 'motorbike',\n",
    "                  'paragliding', 'rhino', 'scooter-gray', 'swing']\n",
    "\n",
    "TRAIN_SEQUENCES = ['bear', 'bmx-bumps', 'boat', 'breakdance-flare', 'bus', \n",
    "                   'car-turn', 'dance-jump', 'dog-agility', 'drift-turn', \n",
    "                   'elephant', 'flamingo', 'hike', 'hockey', 'horsejump-low', \n",
    "                   'kite-walk', 'lucia', 'mallard-fly', 'mallard-water', \n",
    "                   'motocross-bumps', 'motorbike', 'paragliding', 'rhino', \n",
    "                   'rollerblade', 'scooter-gray', 'soccerball', 'stroller',\n",
    "                   'surf', 'swing', 'tennis', 'train']\n",
    "\n",
    "VAL_SEQUENCES = ['blackswan', 'bmx-trees', 'breakdance', 'camel', 'car-roundabout',\n",
    "                 'car-shadow', 'cows', 'dance-twirl', 'dog', 'drift-chicane', \n",
    "                 'drift-straight', 'goat', 'horsejump-high', 'kite-surf', 'libby', \n",
    "                 'motocross-jump', 'paragliding-launch', 'parkour', 'scooter-black', \n",
    "                 'soapbox']\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "#0: bear\n",
      "Start online training...\n",
      "Finished online training...\n",
      "Create new OSVOS model...\n",
      "Constructing OSVOS architecture..\n",
      "Initializing weights..\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n",
      "x.dtype=torch.float64\n",
      "edge_index.dtype=torch.float64\n",
      "edge_attr.dtype=torch.float64\n",
      "y.dtype=torch.float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-839c4c932b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mLAYER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mSKIP_SEQUENCES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_SEQUENCES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_SEQUENCES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                   train=True)\n\u001b[0m",
      "\u001b[0;32m~/in2364-adl4cv/pg_datasets/davis_2016.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, contours_folders_path, images_folders_path, translations_folders_path, layer, k, skip_sequences, train_sequences, val_sequences, train, transform, pre_transform)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDAVIS2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/in2364-adl4cv/pg_datasets/davis_2016.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# Get data and append it to corresponding data_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_filter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/in2364-adl4cv/pg_datasets/create_data.py\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(contour, translation, img_path, new_model, k)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# x = get_OSVOS_feature_vectors(contour)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_OSVOS_feature_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x.dtype={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/in2364-adl4cv/pg_datasets/create_data.py\u001b[0m in \u001b[0;36mget_OSVOS_feature_vectors\u001b[0;34m(key_point_positions, img_path, new_model)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#Extract vector out of output tensor depending on keypoint location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#Compute receptive fields for every feature vector. --> Select feature vector which receptive field center is closest to key point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = DAVIS2016(PYTORCH_GEOMETRIC_DAVIS_2016_DATASET_PATH, \n",
    "                  CONTOURS_FOLDERS_PATH, IMAGES_FOLDERS_PATH, TRANSLATIONS_FOLDERS_PATH, \n",
    "                  LAYER, K, \n",
    "                  SKIP_SEQUENCES, TRAIN_SEQUENCES, VAL_SEQUENCES,\n",
    "                  train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = DAVIS2016(PYTORCH_GEOMETRIC_DAVIS_2016_DATASET_PATH, \n",
    "                CONTOURS_FOLDERS_PATH, IMAGES_FOLDERS_PATH, TRANSLATIONS_FOLDERS_PATH, \n",
    "                LAYER, K, \n",
    "                SKIP_SEQUENCES, TRAIN_SEQUENCES, VAL_SEQUENCES,\n",
    "                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[0]\n",
    "for key, item in data:\n",
    "    print(key, type(item))\n",
    "\n",
    "print(data)\n",
    "print(data.num_features)\n",
    "print(data.num_nodes)\n",
    "y = data.y\n",
    "print(y.shape)\n",
    "y = y.flatten()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(in_channels, 256)\n",
    "        self.conv2 = GCNConv(256, 512)\n",
    "        self.conv3 = GCNConv(512, 1024)\n",
    "        \n",
    "        self.lin1 = nn.Linear(1024, 512)\n",
    "        self.lin2 = nn.Linear(512, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)   \n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)    \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(n_epochs):\n",
    "\n",
    "    # prepare the net for training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "\n",
    "            # convert variables to floats for regression loss\n",
    "            # data = data.type(torch.FloatTensor)\n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            out = model(data)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(out, data.y.flatten)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to the running_loss, use .item()\n",
    "            if batch_i % 10 == 9:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1,\n",
    "                                                                   batch_i+1,\n",
    "                                                                   running_loss / (len(train_loader)*epoch+batch_i)))\n",
    "            \n",
    "            #return images, key_pts, output_pts\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    \n",
    "# Load model and run the solver\n",
    "model = Net(in_channels=train[0].num_features, \n",
    "            out_channels=train[0].num_nodes * 2)\n",
    "model.float()\n",
    "print(model)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-6, nesterov=True)\n",
    "train_net(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Net(in_channels=train[0].num_features, \n",
    "            out_channels=train[0].num_nodes * 2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
